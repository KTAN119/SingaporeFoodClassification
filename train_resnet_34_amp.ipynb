{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
    "    Reference:\n",
    "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
    "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        epsilon (float): weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, epsilon=0.1, device='cpu'):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        # targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data, 1)# for mldg da\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)#for zzd\n",
    "        targets = targets.to(self.device)\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (-Variable(targets) * log_probs).mean(0).sum()\n",
    "        return loss\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "        dist = dist + dist.t()\n",
    "        dist.addmm_(1, -2, inputs, inputs.t())\n",
    "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=2048, device='cpu'):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim)).to(self.device)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (num_classes).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        classes = classes.to(self.device)\n",
    "        \n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.data.eq(classes.expand(batch_size, self.num_classes))\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, file, transform=None, mode='train'):\n",
    "        self.transforms = transform\n",
    "        self.mode = mode\n",
    "        with open(file, 'r') as f:\n",
    "            self.image_list = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = None\n",
    "        if self.mode == 'train':\n",
    "            image, label = self.image_list[index].split('\\n')[0].split('\\t')\n",
    "            label = int(label)\n",
    "        else:\n",
    "            image = self.image_list[index].split('\\n')[0]\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        if self.mode == 'train':\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.Pad(10, 10),\n",
    "                transforms.RandomRotation(45),\n",
    "                transforms.RandomCrop((224, 224)),\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction, ground_truth):\n",
    "    num_correct = (np.array(prediction) == np.array(ground_truth)).sum()\n",
    "    return num_correct / len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/train.txt', transform=transforms_train)\n",
    "val_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/val.txt', transform=transforms_test)\n",
    "test_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/test.txt', transform=transforms_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "train_model = models.resnet34(pretrained=True)\n",
    "train_model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "model_str = 'resnet34'\n",
    "output_dir = 'checkpoint_' + model_str\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ce_loss = CrossEntropyLabelSmooth(num_classes = num_classes, device = device)\n",
    "optimizer = torch.optim.Adam(train_model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.load_state_dict(torch.load('checkpoint_resnet34/resnet34_50.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for forward pass without AMP: 0.004930734634399414\n",
      "Time taken for forward pass with AMP: 0.005903005599975586\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_model\n",
    "train_model.eval()\n",
    "temp = torch.rand([1, 3, 224, 224])\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    out = train_model(temp)\n",
    "    end = time.time()\n",
    "    print('Time taken for forward pass without AMP: {}'.format(end - start))\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        start = time.time()\n",
    "        out = train_model(temp)\n",
    "        end = time.time()\n",
    "        print('Time taken for forward pass with AMP: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59d55efc3955e58528d889e5fb54ed26f80617db376d5c1d40ac8e87cb109911"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
