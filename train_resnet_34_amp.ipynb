{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
    "    Reference:\n",
    "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
    "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        epsilon (float): weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, epsilon=0.1, device='cpu'):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        # targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data, 1)# for mldg da\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)#for zzd\n",
    "        targets = targets.to(self.device)\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (-Variable(targets) * log_probs).mean(0).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, file, transform=None, mode='train'):\n",
    "        self.transforms = transform\n",
    "        self.mode = mode\n",
    "        with open(file, 'r') as f:\n",
    "            self.image_list = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = None\n",
    "        if self.mode == 'train':\n",
    "            image, label = self.image_list[index].split('\\n')[0].split('\\t')\n",
    "            label = int(label)\n",
    "        else:\n",
    "            image = self.image_list[index].split('\\n')[0]\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        if self.mode == 'train':\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.Pad(10, 10),\n",
    "                transforms.RandomRotation(45),\n",
    "                transforms.RandomCrop((224, 224)),\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction, ground_truth):\n",
    "    num_correct = (np.array(prediction) == np.array(ground_truth)).sum()\n",
    "    return num_correct / len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/train.txt', transform=transforms_train)\n",
    "val_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/val.txt', transform=transforms_test)\n",
    "test_ds = FoodDataset('/media/ntu/volume2/home/s121md302_07/food/data/test.txt', transform=transforms_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "train_model = models.resnet34(pretrained=True)\n",
    "train_model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "model_str = 'resnet34'\n",
    "output_dir = 'checkpoint_' + model_str\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ce_loss = CrossEntropyLabelSmooth(num_classes = num_classes, device = device)\n",
    "optimizer = torch.optim.Adam(train_model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.load_state_dict(torch.load('checkpoint_resnet34/resnet34_50.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for forward pass without AMP: 0.03173828125\n",
      "Time taken for forward pass with AMP: 0.015445709228515625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_model.eval()\n",
    "temp = torch.rand([1, 3, 224, 224])\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    out = train_model(temp)\n",
    "    end = time.time()\n",
    "    print('Time taken for forward pass without AMP: {}'.format(end - start))\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        start = time.time()\n",
    "        out = train_model(temp)\n",
    "        end = time.time()\n",
    "        print('Time taken for forward pass with AMP: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, gc\n",
    "\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume2/home/s121md302_07/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/cuda/memory.py:274: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.00s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 26.669 sec\n",
      "Max memory used by tensors = 10642266112 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "train_model.train()\n",
    "train_model.to(device)\n",
    "start_timer()\n",
    "for ep in range(epoch):\n",
    "    start = time.time()\n",
    "    for img, label in tqdm(train_dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output= train_model(img)\n",
    "        loss = ce_loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end = time.time()\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Time taken for 1 epoch without AMP: {}'.format(sum(time_list)/len(time_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 26.453 sec\n",
      "Max memory used by tensors = 7173105152 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "train_model.train()\n",
    "train_model.to(device)\n",
    "epoch = 10\n",
    "start_timer()\n",
    "for ep in range(epoch):\n",
    "    for img, label in tqdm(train_dl):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output= train_model(img)\n",
    "            assert output.dtype is torch.float16\n",
    "            loss = ce_loss(output, label)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    end = time.time()\n",
    "end_timer_and_print(\"Default precision:\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Time taken for 1 epoch with AMP: {}'.format(sum(time_list)/len(time_list)))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59d55efc3955e58528d889e5fb54ed26f80617db376d5c1d40ac8e87cb109911"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
